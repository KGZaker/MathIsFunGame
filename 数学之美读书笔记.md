<a name="content">目录</a>

[数学之美读书笔记](#title)
- [1. 从文本分类的角度理解奇异值分解(SVD)](#understand-svd-in-view-of-text-classification)
    - [1.1. SVD算法](#svd-algorithmn)
    - [1.2. 理解SVD分解所得三个矩阵的含义](#understand-meaning-of-3-constituent-matrixs)
- [2. 关键词权重的科学度量TF-IDF](#proper-measure-term-weight-tf-idf)
- [3. 推荐系统的奥秘](#mystery-recommendation-system)
    - [3.1. 基于用户数据：协同过滤算法](#recommendation-system-based-on-users-data)
    - [3.2. 基于内容：摆脱协同过滤算法对用户数据的过分依赖](#recommendation-system-based-on-content)
    - [3.3. 相似度到底是怎么算出来的？](#how-to-measure-similarity)
- [4. 用余弦相似度进行文本分类](#text-classification-using-cosine-similarity)


<h1 name="title">数学之美读书笔记<h1>

<a name="understand-svd-in-view-of-text-classification"><h2>1. 从文本分类的角度理解奇异值分解(SVD) [<sup>目录</sup>](#content)</h2></a>

<a name="svd-algorithmn"><h3>1.1. SVD算法 [<sup>目录</sup>](#content)</h3></a>

矩阵A可以如下分解成三个矩阵的乘积：

$$
A_{MN}=X_{MM} \times B_{MN} \times Y_{NN}
$$

其中X是一个酉矩阵 (Unitary Matrix)，Y则是一个酉矩阵的共轭矩阵

> 与其共轭矩阵转置相乘等于单位阵的矩阵是酉矩阵，因此酉矩阵及其共轭矩阵都是方阵

B是一个对角阵，即只有对角线上是非0值

维基百科上给出了下面的例子：

$$
A_{4 \times 5}=
\begin{bmatrix}
1 & 0 & 0 & 0 & 2 \\\\
0 & 0 & 3 & 0 & 0 \\\\
0 & 0 & 0 & 0 & 0 \\\\
0 & 4 & 0 & 0 & 0
\end{bmatrix}
\quad
X_{4 \times 5}=
\begin{bmatrix}
0 & 0 & 1 & 0 \\\\
0 & 1 & 0 & 0 \\\\
0 & 0 & 0 & -1 \\\\
1 & 0 & 0 & 0
\end{bmatrix}
\quad
B_{4 \times 5}=
\begin{bmatrix}
4 & 0 & 0 & 0 & 0 \\\\
0 & 3 & 0 & 0 & 0 \\\\
0 & 0 & \sqrt{5} & 0 & 0 \\\\
0 & 0 & 0 & 0 & 0
\end{bmatrix}
\quad
Y_{5 \times 5}=
\begin{bmatrix}
0 & 1 & 0 & 0 & 0 \\\\
0 & 0 & 1 & 0 & 0 \\\\
\sqrt{0.2} & 0 & 0 & 0 & \sqrt{0.8} \\\\
-\sqrt{0.8} & 0 & 0 & 0 & \sqrt{0.2}
\end{bmatrix}
$$

那么如何进行奇异值分解呢？

> 一般分两步进行：
>
> （1）将矩阵A变换成一个双对角矩阵（除了两行对角线元素非零，剩下的都是零），这个过程的计算量为$O(MN^2)$，如果矩阵是稀疏的，那么可以大大缩短计算时间；
>
> （2）将双对角矩阵变成奇异值分解的三个矩阵。这一步的计算量只是第一步的零头；

奇异值分解的一个重要目的是进行数据的低维度表示，即将$A_{MN}=X_{MM} \times B_{MN} \times Y_{NN}$ 转换为 $A_{MN}=X_{Mn} \times B_{nn} \times Y_{nN}\quad(n\le min\{M,N\}$

直观一点的：

<p align="center"><img src=./picture/Beauty-of-Mathmatics-Note-SVD-1.png height=200 /></p>

降维表示：

<p align="center"><img src=./picture/Beauty-of-Mathmatics-Note-SVD-2.png height=200 /></p>

如何从原始的矩阵分解结果得到它的降维表示呢？为什么可以这么做？

> 由于对角矩阵B的对角线上的元素的很多值相对于其他的值非常小，或者干脆为0，故而可以省略，例如，当B为如下情况时：
>
> $$B_{4 \times 5}=\begin{bmatrix}4 & 0 & 0 & 0 & 0 \\\\ 0 & 3 & 0 & 0 & 0 \\\\ 0 & 0 & \sqrt{5} & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 \end{bmatrix}$$
>
> 则可以对B进行简化，省略都是0的行和列，得到B'：
>
> $$B'_{3 \times 3}=\begin{bmatrix}4 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & \sqrt{5} \end{bmatrix}$$
>
> 那么想对应地，X保留前n列，Y保留前n行

<a name="understand-svd-in-view-of-text-classification"><h3>1.2. 理解SVD分解所得三个矩阵的含义 [<sup>目录</sup>](#content)</h3></a>

用一个打矩阵来表示成千上万篇文章和几十上百万个词的关联性

在这个矩阵中，每行表示一个词，每列表示一篇文章，如果有M篇文章，N个词，则可以得到一个MxN的矩阵：

$$
A=
\begin{bmatrix}
a_{11} & a_{12} & ... & a_{1M} \\\\
a_{21} & a_{22} & ... & a_{2M} \\\\
... & ... \\\\
a_{N1} & a_{N2} & ... & a_{NM} \\\\
\end{bmatrix}
$$

其中$a_{ij}$表示的是第j篇文章的第i个词的加权词频（比如用词的TF-IDF）。一般来说这个矩阵会非常非常大

对A进行奇异值分解后：

<p align="center"><img src=./picture/Beauty-of-Mathmatics-Note-SVD-2.png height=200 /></p>

原始矩阵A的元素个数为$M \times N$，奇异值分解后得到的上小矩阵的元素总是为$M \times n + n \times n + n \times N = n(M+N+n)$，一般情况下$M \times N \gg n(M+N+n)$，这使得数据的存储量和计算量都远小于原始矩阵

这三个矩阵都有非常清晰的物理含义：

- 矩阵X

    矩阵X是对词进行分类的结果，它的每一行表示一个词，每一列表示一个语义相近的词类，或者称为语义类

    这一行的每个非零元素表示这个词在每个语义类的重要性（或者说是相关性），例如：

    $$X=\begin{bmatrix}0.7 & 0.15 \\\\ 0.22 & 0.49 \\\\ 0 & 0.92\end{bmatrix}$$

    则第一个词与第一个词类最相关，而与第二个此类的关系比较弱，以此类推

- 矩阵Y

    矩阵Y是对文本的分类结果，它的每一列对应一篇文章，每一行对应一个文章主题

    这一列的每个非零元素表示这篇文章在每个的主题重要性（或者说是相关性），例如：

    $$Y=\begin{bmatrix}0.7 & 0.15 & 0.22 & 0.39 \\\\ 0 & 0.92 & 0.08 & 0.53\end{bmatrix}$$

    则第一篇文章很明显属于第一个主题，第二篇文章和第二个主题很相关，以此类推

- 矩阵B

    中间的矩阵则表示词的类和文章的类之间的相关性，例如

    $$B=\begin{bmatrix}0.7 & 0.21 \\\\ 0.18 & 0.63 \end{bmatrix}$$

    则第一个词的语义类与第一个主题相关，而第二个主题相关性较弱，而第二个词的语义类则相反

<a name="proper-measure-term-weight-tf-idf"><h2>2. 关键词权重的科学度量TF-IDF [<sup>目录</sup>](#content)</h2></a>

以短语“原子能的应用”为例，可以拆分成三个关键词：“原子能”、“的”和“应用”

主要思想：词出现次数较多的网页应该比它们出现较少的网页相关性高

> 缺点一：篇幅长度的影响
>
> 解决方案：根据篇幅长度，对关键词次数进行归一化，即$TF_c=\frac{n_c}{N}$，称为关键词的“单文本词频” (Term Frequency)

此时，要度量网页与查询之间的相关性，一个简单直接的方法就是：直接使用各个关键词在网页中出现的总词频

若查询包含N个关键词$w_1,w_2,...,w_N$，它们在某个特定网页中的词频分别是$TF_1,TF_2,...,TF_N$，则这个网页的与该查询之间的相关性为：

$$TF_1+TF_2+...+TF_N$$

> 缺点二：“停止词”的干扰
>
> 解决方案：在度量相关性时，不考虑这些词的频率

> 缺点三：没有考虑不同关键词的信息量。例如，“应用”是个通用的词，而“原子能”是个很专业的词，后者在相关性评估中应该比前者更重要
>
> 解决方案：对每个关键词施加一个权重，这个权重的设定必须满足：
>
> - 预测主题的能力强，则权重大，否则，权重小；
>
> - 停止词权重为0——不需要对第二个缺点做特殊的处理，在这里就顺带解决了第二个问题；

这样查询与某个网页之间的相关性就变成了：

$$TF_1·IDF_1+TF_2·IDF_2+...+TF_N·IDF_N$$

其中，$IDF_i$是第i个关键词对应的权重

那么具体该如何得到$IDF_i$呢？

> 基于这样的常识：如果一个关键词只在很少的网页中出现，通过它就容易锁定搜索目标，它的权重就应该大；反之，如果一个词在大量的网页中都出现，看到它仍然难以确定要找什么内容，那么它的权重应该小
>
> 因此，假定一个关键词$w$在$D_w$个网页中出现过，那么$D_w$越小，$w$的权重就越大
>
> 在信息检索中，使用最多的权重是“逆文本频率指数” (Inverse Document Frequency, IDF)
>
> $$IDF_w=log(\frac{D}{D_w})$$

<a name="mystery-recommendation-system"><h2>3. 推荐系统的奥秘 [<sup>目录</sup>](#content)</h2></a>

<a name="recommendation-system-based-on-users-data"><h3>3.1. 基于用户数据：协同过滤算法 [<sup>目录</sup>](#content)</h3></a>

归功于亚马逊工程师的“发明”——“一个客户买了这个东西，那么他也可能买另一个东西”

基本思想：

> 喜好相同的人和人之间有相似的消费/行为模式。喜好这个东西的人，倾向于也喜好另一个

实现的方法为“协同过滤”算法 (collaborative filtering)

下面以音乐推荐系统为例进行说明，基于对用户历史数据的不同侧重，可以分为以下两类应用情景：

> （1）基于用户：对每一个用户的听歌偏好作为向量，计算用户喜好之间的相似度，找到与某个用户X喜好最相似的一个其他用户Y，然后将Y的歌单里有而X没有的歌推荐给X
>
> （2）基于项目（单曲）：将用户对于一首歌的偏好作为向量，计算单曲之间的相似度，若某个用户喜欢/收藏了某一首歌，则将于这首歌相似的歌曲推荐给这个用户

但是，基于单一协同过滤算法的推荐系统会存在明显的误差：

> - 除了用户及消费模式信息，不涉及被推荐单曲本身的任何信息
>
>   这使得热门音乐币冷门音乐更容易得到推荐，因为前者拥有更多数据
>
>   如果推荐系统只能给出热门歌曲的推荐，往往很难让人感到惊喜
>
> - 而基于项目（单曲）的协同过滤，也有一个问题，就是相似使用模式下的内容异质。
>
>   例如你听了一张新专辑里面全部的歌，但除了主打歌，其他的一些插曲、翻唱曲以及混音曲可能都不是歌手的典型作品，那么协同过滤在这个时候，就会因为这些「噪音」而产生偏差。
>
> - 最大的问题便是“没有数据，一切皆失效”

<a name="recommendation-system-based-on-content"><h3>3.2. 基于内容：摆脱协同过滤算法对用户数据的过分依赖 [<sup>目录</sup>](#content)</h3></a>

在数据量庞大且足够干净的时候，协同过滤算法是非常强大的，但如果作为一个新用户，在数据稀少的情况下，推荐系统该怎么获知我的口味？

可以利用歌曲本身的信息来得到推荐结果，其基本思想是：

> 当你喜欢一首歌时，你会倾向也喜欢同类型的其他歌曲
>
> 不同歌曲有很多不同的属性，用一个向量去描述该单曲的属性，每一个维度的值代表一个属性的定量描述
>
> 按照这些属性，可以计算两首歌曲的相似度

基于内容的推荐算法更像是对协同过滤算法以上缺陷的一种补充——**假如没有大量用户数据，或者想听冷门歌曲，我们就只能从音乐本身寻找答案了**

前面提到，可以根据歌曲的不同维度的属性去构造一个特征向量去描述它，但是可供选择的属性实在是太多了，因此需要构造的特征向量维度过大——可以利用深度学习建立基于音频的推荐模型，通过特征的embedding和降维方法，把这么多特征映射到低维变量空间里

<a name="how-to-measure-similarity"><h3>3.3. 相似度到底是怎么算出来的？ [<sup>目录</sup>](#content)</h3></a>

可以拥有描述相似度的统计量为：欧式距离和余弦相似度

<p align="center"><img src=./picture/Beauty-of-Mathmatics-Note-RecoSys-measure-similarity.png width=600 /></p>

可以看出，在上图中，如果固定B，让A沿着直线OA方向移动，在移动过程中，AB的余弦夹角始终保持不变，而两点之间的绝对距离一直在变化

这种差异使得在使用它们进行相似度描述时，要考虑数据的特性：

> （1）欧式距离：能够突出数值绝对差异，在欧式距离下，用户对歌曲的偏好都可以被认为是一样的分数，可以简化歌曲相似度的计算；
>
> （2）余弦相似度：更多是从用户偏好方向上区分差异

<a name="text-classification-using-cosine-similarity"><h2>4. 用余弦相似度进行文本分类 [<sup>目录</sup>](#content)</h2></a>

- 构造文本特征向量

    在 [《2. 关键词权重的科学度量TF-IDF》](#proper-measure-term-weight-tf-idf) 我们已经讨论过用什么样的特征能够比较好地描述文本的特征信息

    因此，这里我们使用TF-IDF来构造文本特征向量：对于一篇文本中的所有实词，计算它们的TF-IDF值

    比如，比如词汇表有64000个词，其编号和词如下表所示：

    $$\begin{array}{c|c}\hline编号 & 实词 \\\\ \hline 1 & 阿 \\\\ 2 & 啊 \\\\ 3 & 阿斗 \\\\ ... & ... \\\\ \hline \end{array}$$

    对于某一篇文本，这64000个词的TF-IDF值如下：

    $$\begin{array}{c|c}\hline编号 & TF-IDF \\\\ \hline 1 & 0 \\\\ 2 & 0.0034 \\\\ 3 & 0 \\\\ ... & ... \\\\ \hline \end{array}$$

- 基于余弦相似度进行文本分类的两种情况

    （1）我们已经知道一些新闻类别的特征向量$x_1,x_2,...,x_n$

    对任何一个要被分类的文本Y，计算出它与每个特征向量的余弦相似性，基于最近邻算法（Nearest Neighbor）Algorithm），将它归类于于它最相似的那个类别里

    （2）事先并不知道这些文本类别的特征向量

    可以先采用层次聚类得到模糊的类别标签，至于聚类到什么程度停止，需要根据实际情况人为地控制——这样就得到了一个个小类，可以计算出每个小类的特征向量

    这样问题就变成了第一种分类情况了

---

参考资料：

(1) 吴军《数学之美（第二版）》

(2) [网易云音乐首次披露推荐算法: 让单身狗犹如过情人节的日推原来是这样生成的](https://c.m.163.com/news/a/DAKB3SMV05118DFD.html?spss=newsapp&spsw=2&spssid=9374de12adb09dad3dbb52df65782186)
